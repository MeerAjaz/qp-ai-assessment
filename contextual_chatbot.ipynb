{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1f095d9",
   "metadata": {},
   "source": [
    "@ajaz_ahmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ad2b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "import PyPDF2\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from pinecone import Pinecone, PodSpec\n",
    "import re\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c964ca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath =  '../../../../AJAZ_AHMED_3YOE.pdf' # #'../../../../file-sample_1MB.docx'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b36b03",
   "metadata": {},
   "source": [
    "## Project 1\n",
    "```\n",
    "Simple Contextual Chat Bot\n",
    "1. Read a long PDF/ Word Document. \n",
    "2. Build a chat bot that will use the document as a context to answer the question. \n",
    "3. If the answer is not found in the document - it should say I don't know the answer. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d38a2c",
   "metadata": {},
   "source": [
    "### 1. Read a long PDF/ Word Document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc0204f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(file_path):\n",
    "    # Open the PDF file in binary mode\n",
    "    with open(file_path, 'rb') as pdf_file:\n",
    "        # Create a PDF reader object\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "\n",
    "        # Get the total number of pages\n",
    "        num_pages = len(pdf_reader.pages)\n",
    "\n",
    "        # Initialize an empty string to store the extracted text\n",
    "        extracted_text = \"\"\n",
    "\n",
    "        # Loop through each page and extract text\n",
    "        for page_num in range(num_pages):\n",
    "            # Get a specific page\n",
    "            page = pdf_reader.pages[page_num]\n",
    "\n",
    "            # Extract text from the page\n",
    "            page_text = page.extract_text()\n",
    "\n",
    "            # Append the text to the result string\n",
    "            extracted_text += page_text\n",
    "\n",
    "    return extracted_text\n",
    "\n",
    "def read_word_document(file_path):\n",
    "    doc = Document(file_path)\n",
    "    text_content = [paragraph.text for paragraph in doc.paragraphs]\n",
    "    return '\\n'.join(text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6be33e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "if filepath.endswith('.pdf'):\n",
    "    word_content = read_pdf(filepath)\n",
    "else:\n",
    "    word_content = read_word_document(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4065401d",
   "metadata": {},
   "source": [
    "### 2. Build a chat bot that will use the document as a context to answer the question.\n",
    "### 3. If the answer is not found in the document - it should say I don't know the answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b220acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/47091490/4084039- reference\n",
    "\n",
    "def preprocess_text(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    phrase = re.sub(r'\\s', ' ', phrase)\n",
    "    return phrase\n",
    "\n",
    "\n",
    "sentences = preprocess_text(word_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f2892bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the document\n",
    "sentences = nltk.sent_tokenize(sentences.lower())\n",
    "# Create TF-IDF vectors for each sentence in the document\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f459a6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(user_input):\n",
    "    # Preprocess user input\n",
    "    user_input = user_input.lower()\n",
    "\n",
    "    # Vectorize user input\n",
    "    user_vector = tfidf_vectorizer.transform([user_input])\n",
    "\n",
    "    # Calculate cosine similarity scores between user input and document sentences\n",
    "    similarity_scores = cosine_similarity(user_vector, tfidf_matrix).flatten()\n",
    "\n",
    "    # Find the sentence with the highest similarity score\n",
    "    max_similarity_index = similarity_scores.argmax()\n",
    "    max_similarity_score = similarity_scores[max_similarity_index]\n",
    "\n",
    "    # Set a threshold for considering the answer\n",
    "    similarity_threshold = 0.2\n",
    "\n",
    "    if max_similarity_score > similarity_threshold:\n",
    "        # Return the corresponding sentence from the document as the answer\n",
    "        return sentences[max_similarity_index]\n",
    "    else:\n",
    "        # 3. If the answer is not found in the document - it should say I don't know the answer. \n",
    "        return \"I don't know the answer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75c40244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: ajaz ahmed\n",
      "Chatbot: ajaz ahmed +919955546834 |ajazahmeddnr@gmail.com |linkedin |github |leetcode experience machine learning engineer aug 2021 – till date sony india software centre - on the payroll of ignitarium bengalore, india •object detection :- successfully deployed and custom trained yolov3 to aws sagemaker.\n",
      "User: python\n",
      "Chatbot: •python(basic) from hackerrank.\n",
      "User: deep learning\n",
      "Chatbot: •gotfeatured onthewalloffame atiotiot.courses & certificates •appliedaicourseassignments •supervised machine learning course from scaler •keras & tensorflow for deep learning from scaler •deep learning course: deep dive into deep learning from scaler •aws foundations: machine learning basics •aws machine learning terminology and process •deep learning with pytorch course by jovian.\n",
      "User: sony\n",
      "Chatbot: •object detection :- worked on sony alpha 9 iii dslr on autofocus.\n",
      "User: classification\n",
      "Chatbot: I don't know the answer.\n",
      "User: exit\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "    answer = get_answer(user_input)\n",
    "    print(\"Chatbot:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596133fa",
   "metadata": {},
   "source": [
    "## Project 2\n",
    "\n",
    "```\n",
    "Advanced Challenge:\n",
    "- Break down the document into multiple chunks/ paragraphs. \n",
    "- Store them in a vector database like pinecone.  \n",
    "- When you ask a question find out the top 3 chunks that will likely have the answer to the question using semantic similarity search. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a059146b",
   "metadata": {},
   "source": [
    "- Break down the document into multiple chunks/ paragraphs.\n",
    "     - This part is already done since we've broken documents into multiple chunks(sentences) and converted into tfidf matrix. From here onwards would move the matrix to pinecone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc3ea8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_to_pinecone(embedded_vectors, index_name, api_key):\n",
    "    '''\n",
    "    This function push the vectors to pinecone.\n",
    "    embedded_vectors: vectors generated using embedding, type: sparse_matrix.\n",
    "    index_name: name of the index to create.\n",
    "    api_key: api_key to access pinecone account.\n",
    "    '''\n",
    "    # Initialize Pinecone client\n",
    "    pinecone = Pinecone(api_key=api_key)\n",
    "    dimension = tfidf_matrix.shape[1]  \n",
    "    spec=PodSpec(environment=\"gcp-starter\", pod_type=\"us-central-1\")\n",
    "\n",
    "\n",
    "    # create an index    \n",
    "    pinecone.create_index(index_name, dimension, spec)\n",
    "\n",
    "    # Connect to the existing or newly created index\n",
    "    index = pinecone.Index(index_name)\n",
    "    \n",
    "    # Ids of vectors\n",
    "    vector_ids = [f\"vector_{i}\" for i in range(embedded_vectors.shape[0])]\n",
    "    # sparse_matrix converting into list.\n",
    "    values = [{'id': vector_id, 'values': value.toarray().flatten().tolist()} for vector_id, value in zip(vector_ids, tfidf_matrix)]\n",
    "\n",
    "\n",
    "    # Upsert vector to Pinecone index\n",
    "    for row in tqdm.tqdm(values):\n",
    "        index.upsert(vectors=[row])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a83f52a",
   "metadata": {},
   "source": [
    "- Store them in a vector database like pinecone.\n",
    "    - This part is done by below function(push_to_pinecone)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a51687b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [00:16<00:00,  1.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# upsert all the vectors to pinecone\n",
    "api_key = 'enter_your_api_key'\n",
    "index_name = 'demo'\n",
    "push_to_pinecone(tfidf_matrix, index_name, api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db7c1908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def text_to_vectors(text):\n",
    " \n",
    "    # Preprocess the document\n",
    "    sentence = nltk.sent_tokenize(text.lower())\n",
    "    # Create TF-IDF vectors for each sentence in the document\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.transform(sentence)\n",
    "    vectors = tfidf_matrix.toarray().flatten().tolist()\n",
    "    return vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9fa89f",
   "metadata": {},
   "source": [
    "- When you ask a question find out the top 3 chunks that will likely have the answer to the question using semantic similarity search. \n",
    "    - This part is being done here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4365190a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 Chunks: ['vector_16', 'vector_0', 'vector_17']\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Pinecone client\n",
    "pinecone = Pinecone(api_key=api_key)\n",
    "\n",
    "# Connect to the existing or newly created index\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "# Function to perform semantic similarity search\n",
    "def find_similar_chunks(question, top_k=3):\n",
    "    # Convert question to vector using the same vectorization method used for sentences\n",
    "    sentence = preprocess_text(question)\n",
    "   \n",
    "    vectors = tfidf_vectorizer.transform([sentence])\n",
    "    vectors = vectors.toarray().flatten().tolist()\n",
    "    # Perform semantic similarity search\n",
    "    results = index.query(vector=vectors, top_k=top_k)\n",
    "    # Extract top_k chunks\n",
    "    top_chunks = [result.id for result in results.matches]\n",
    "\n",
    "    return top_chunks\n",
    "\n",
    "\n",
    "# Example usage\n",
    "question = \"who is ajaz ahmed? Is he deep learning engineer?\"\n",
    "top_chunks = find_similar_chunks(question)\n",
    "\n",
    "print(\"Top 3 Chunks:\", top_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebb0b05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interview",
   "language": "python",
   "name": "interview"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
